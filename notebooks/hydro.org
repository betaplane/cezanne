* Note: this is not the same as the hydro.ipynb (yet)
* links
** http://www.hidro-limari.info/
** https://earth.boisestate.edu/drycreek/data/

#+begin_src ipython :results silent :session
  import requests, os
  from io import StringIO
  from functools import partial
  from tqdm import tqdm
  from data import DataConf, Unicode

  class DryCreek(DataConf):
      file_name = Unicode().tag(config=True)
      column_names = {
          'T': ['AirTemperature-C', 'AirTemperature_C'],
          'nrad': ['NetRadiation-Watts/m2','NetRadiation_W/m2', 'NetRadiation_watts/m2'],
          'ppt': ['Precipitation-mm','Precipitation_mm'],
          'rh': ['RelativeHumidity-%', 'RelativeHumidity_percent'],
          'snwd': ['SnowDepth-cm', 'SnowDepth_cm'],
          'rs': ['SolarRadiation-Watts/m2','SolarRadiation_W/m2', 'SolarRadiation_watts/m2'],
          'dd': ['WindDirection-Degree', 'WindDirection_degree'],
          'vv': ['WindSpeed-m/s','WindSpeed_m/s']
      }
      urls = {
          'LDP': 'http://icewater.boisestate.edu/boisefront-products/historical/dcew_ldp/Weather/Hourly%20Summary%20Data/LDP_HrlySummary_{:04d}.csv',
          'SCR': 'http://icewater.boisestate.edu/boisefront-products/historical/dcew_scr/weather/hourly%20summary%20data/SCR_HrlySummary_{:04d}.csv',
          'BRW': 'http://icewater.boisestate.edu/boisefront-products/historical/dcew_br/Weather/Hourly%20Summary%20Data/BRW_HrlySummary_{:04d}.csv',
          'LW': 'http://icewater.boisestate.edu/boisefront-products/historical/dcew_lw/Weather/Hourly%20Summary%20Data/LowerWeather_HrlySummary_{:04d}.csv'
      }

      def get(self, station, years, **kwargs):
          f = partial(self._get_url, self.urls[station])
          self.data = [f(y, **kwargs) for y in tqdm(years)]
          self.data = pd.concat(self.data)
          self.data = self.data.loc[self.data.index.notnull()].sort_index()
          if len(self.data.columns.difference([i for j in self.column_names.values() for i in j])) > 0:
              print("Data has columns not present in column_names")
          else:
              self.merge(station)

      def merge(self, name):
          df = pd.concat([self._merge(self.data, n, c)for n, c in self.column_names.items()]).dropna(0, 'all')
          setattr(self, name, df.loc[df.index.notnull()])

      def store(self, name):
          getattr(self, name).to_hdf(os.path.join(self.path, self.file_name), name)

      @staticmethod
      def _get_url(url, y, skiprows=19, parse_dates=True, index_col='DateTime', na_values=-6999):
          r = requests.get(url.format(y))
          r.raise_for_status()
          s = StringIO(r.text)
          while next(s)[:8] != index_col:
              p = s.seek(0, 1)
          s.seek(p)
          return pd.read_csv(
              s, parse_dates=parse_dates, index_col=index_col, na_values=na_values
          ).dropna(1, 'all')

      @staticmethod
      def _merge(df, name, cols):
          c = df.columns.intersection(cols)
          assert df[c].notnull().sum(1).max() == 1
          return df[c].sum(1).to_frame(name)

#+end_src


#+begin_src ipython :results silent :session
  column_names = {
      'T': ['AirTemperature-C', 'AirTemperature_C'],
      'nrad': ['NetRadiation-Watts/m2','NetRadiation_W/m2', 'NetRadiation_watts/m2'],
      'ppt': ['Precipitation-mm','Precipitation_mm'],
      'rh': ['RelativeHumidity-%', 'RelativeHumidity_percent'],
      'snwd': ['SnowDepth-cm', 'SnowDepth_cm'],
      'rs': ['SolarRadiation-Watts/m2','SolarRadiation_W/m2', 'SolarRadiation_watts/m2'],
      'dd': ['WindDirection-Degree', 'WindDirection_degree'],
      'vv': ['WindSpeed-m/s','WindSpeed_m/s']
  }
#+end_src

#+begin_src ipython :results silent :session
  def merge(df, name, cols):
      c = df.columns.intersection(cols)
      assert df[c].notnull().sum(1).max() == 1
      return df[c].sum(1).to_frame(name)

  ldp = pd.concat([merge(ldp, n, c)for n, c in column_names.items()]).dropna(0, 'all')
  
#+end_src

#+begin_src ipython :results silent :session
  import requests, re
  from bs4 import BeautifulSoup as soup
  from io import StringIO
  import pandas as pd

  r = requests.get('https://earth.boisestate.edu/drycreek/data/lower-gage/')
  r.raise_for_status()
  s = soup(r.text, 'html.parser')
  def f(a):
      r = requests.get(a.attrs['href'])
      r.raise_for_status()
      return pd.read_csv(StringIO(r.text), skiprows=18, parse_dates=True, index_col='DateTime', na_values=-6999)

  df = pd.concat([f(a) for a in s.find_all('a', {'href': re.compile('HrlySummary')})]).dropna('all', 1)
#+end_src

#+begin_src ipython :results silent :session
  import pandas as pd
  import numpy as np
  # df is in ~/Documents/data/hydro/DryCreekBoise.h5
  df = pd.read_hdf('/home/arno/Documents/data/hydro/DryCreek/DryCreekBoise.h5', 'LG')
  df = df[df.index.notnull()].replace(-6934, np.nan).sort_index()
  d = df['2003':'2016'].iloc[:, 0] # no duplicates etc
  d = d['2011-08':'2014-04']       # no gaps
#+end_src

* Fourier Transforms
** idea was to test usefulness for gap filling
** not very useful, although maybe lsq or MEM approaches might offer more
*** part of the problem is the finite-length effect of the DFT (implied periodicity on the unit circle)
*** wavelets probably a much better idea
** attempt at a naive least-square implementation
#+begin_src ipython :results silent :session
  import numpy as np

  N = 256
  C = 16
  s = 1
  t = np.linspace(0, C, N)
  y = np.sin(2* np.pi * t) + np.random.rand(N) * s

  f = np.fft.fftfreq(N)
  f = f[f>0]

  x = 2 * np.pi * np.linspace(0, 1, N).reshape((-1, 1))
  X = x / f
  X = np.hstack((np.sin(X), np.cos(X)))
  l = np.linalg.lstsq(X, y)
#+end_src

** DFT by hand
*** to compare to just setting missing values to 0 (not tested yet)
#+begin_src ipython :results silent :session
  n = np.arange(N).reshape(-1, 1)
  n1 = np.r_[n[:100], n[110:]]
  k = np.arange(-N/8, N/8)
  F = np.exp(-2j * np.pi * n1 * k / N)
  G = np.exp(2j * np.pi * n * k / N)
  yh = G.dot(np.r_[y[:100], y[110:]].dot(F)) / N
#+end_src

* regressions etc
** tensorflow linear regression
#+begin_src ipython :results silent :session
  import tensorflow as tf

  def grdesc(features, labels, learn, steps):
      gr = tf.Graph()
      with gr.as_default():
          x = tf.placeholder(tf.float64)
          targ = tf.placeholder(tf.float64)
          a = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          b = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          # here I add a positivity constraint in a somewhat ad-hoc fashin
          # (restoring positivity of innovations at every timestep)
          y = a + b * x
          z = y - tf.reduce_min(targ - y)
          loss = tf.losses.mean_squared_error(targ, z)
          opt = tf.train.GradientDescentOptimizer(learn).minimize(loss)
      with tf.Session(graph=gr) as s:
          tf.global_variables_initializer().run(session=s)
          for i in range(steps):
              r = s.run([opt, loss], {x: features, targ: labels})
              if i % 100 == 0:
                  print(i, r)
          print(i, r)
          return s.run([a, b])
#+end_src

** BayesPy Bayesian linear regression
#+begin_src ipython :results silent :session
  import bayespy as bp
  from statsmodels.tools import add_constant

  class BayesLinReg(object):
      def __init__(self, x, steps=1000):
          self.B = bp.nodes.GaussianARD([0, 1], 1e-6, shape=(2,))
          self.F = bp.nodes.Dot(self.B, add_constant(x[:-1]))
          self.tau = bp.nodes.Gamma(1e-3, 1e-3)
          self.Y = bp.nodes.GaussianARD(self.F, self.tau)
          self.Y.observe(x[1:])
          self.Q = bp.inference.VB(self.Y, self.B, self.tau)
          self.Q.update(repeat=steps)
#+end_src

** Edward Bayesian linear regression
#+begin_src ipython :results silent :session
    import edward as ed
    import tensorflow as tf

    class BLM(object):
        def __init__(self, x, steps=500, K=2):
            self.gr = tf.Graph()
            tf.InteractiveSession(graph=self.gr)
            with self.gr.as_default():
                N = len(x) - 1
                X = tf.placeholder(tf.float32, [N])
                w = ed.models.Normal(loc=tf.ones([K, 1]), scale=tf.ones([K, 1]))

                r = ed.models.Normal(loc=tf.matmul(w, tf.expand_dims(X, 0)), scale=tf.ones(1))
                z = ed.models.Normal(loc=tf.ones((K, N)), scale=tf.ones(1))
                y = ed.models.Normal(loc=tf.reduce_sum(r * z, 0), scale=tf.ones(1))

                self.qw = ed.models.Normal(loc=tf.get_variable("qw/loc", [K, 1]),
                                           scale=tf.nn.softplus(tf.get_variable("qw/scale", [K, 1])))
                self.qz = ed.models.Normal(loc=tf.get_variable("qz/loc", [K, N]),
                                           scale=tf.nn.softplus(tf.get_variable("qz/scale", [1])))
                self.infer = ed.KLqp({w: self.qw, z: self.qz}, data={
                    X: x[:-1],
                    y: x[1:]
                })
                self.infer.run(n_samples=5, n_iter=steps)
#+end_src


* Ad-hoc analyses
#+begin_src ipython :results silent :session
  from data import GDAL
  from cartopy.io.shapereader import Reader
  DEM = GDAL.GeoTiff('/home/arno/Documents/data/hydro/DryCreek/DCEW-DEMclip.tif')
  stream_vec = Reader('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.shp')
  stream_raster = GDAL.GeoTiff('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.tif')
#+end_src

#+begin_src ipython :results raw :session :savefig catchment.png
  fig, ax = plt.subplots(subplot_kw={'projection': DEM.cartopy}, figsize=(8, 8))
  DEM.pcolormesh(ax, background={}, cmap='terrain')
  stream_raster.pcolormesh(ax, background={}, cmap='Dark2')
  px, py = zip(*[(p.x, p.y) for p in stream_vec.geometries()])
  ax.scatter(px, py, facecolor='r')
#+end_src

#+ATTR_ORG: :width 600
#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/catchment.png]]

#+begin_src ipython :results raw :session :savefig alpha_hist.png
  x = d.values
  z = x[1:] / x[:-1]
  fig, axs = plt.subplots(1, 2, figsize=(12, 5))
  ax = axs[0]
  h = ax.hist(z[(z>.98) & (z<1.02)], 1000, color='chartreuse')
  ax.set_xlim(.98, 1.02)
  ax.set_ylim(0, 50)

  z = np.log(z)
  z = 1 / z[z!=0]
  ax = axs[1]
  ax.hist(z[np.isfinite(z)], 1000, color='orangered')
  ax.set_xlim(-150, 150)
  # ax.set_ylim(0, 200)
#+end_src


#+CAPTION: Histograms of "local AR(1) coefficients": x_k / x_{k-1}, where x_k refers to any index in the discharge time series. Left: raw (count on 1 is 14691, ~60% of data). Right: transformed as 1 / log(alpha), which corresponds the e-folding time (in basic units of the timeseries, which is 1h) of an exponential growth/decay process.

#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/alpha_hist.png]]

#+begin_src ipython :results raw :session
  # count in bin centered on one (which is the max count)
  np.max(h[0])
#+end_src

#+RESULTS:
: 14691.0

** Notes
*** Computing the quotient timeseries $\alpha_k = q_{k+1} / q_k$ can be seen as computing a local autoregressive ($AR(1)$) coefficient. While the individual coefficients are not very informative, its distribution is.
*** Furthermore, the inverse of the log of the quotient series ($ln \alpha_k^{-1}$) gives the corresponding e-folding time of a moving-average impulse response.
*** The histograms clearly show 2-3 modes, corresponding to 2-3 major subbasins of the Dry Creek catchment.
*** $\alpha_k < 1$ (or $ln \alpha_k^{-1}<0$) corresponds to the receeding limb of the hydrograph, whereas values >1 (>0) correspond to the ascending limb
*** However, in terms of time series modeling, $\alpha_k>1$ are unphysical; this would imply water levels increasing by themselves rather than because of rainfall.
*** Rainfall would correspond to the innovations of the AR / MA process, but with $\alpha_k$s below 1
*** Note also that the quotient series is very unstable and has very large values at times of very low flow.
**** Some form of regularization/smoothing is necessary for anything more informative.
*** The e-folding time will be influenced by (???):
**** the distance the water has to travel
**** the intensity of the rain

#+begin_src ipython :results silent :session
  def grdesc(features, labels, learn, steps):
      gr = tf.Graph()
      with gr.as_default():
          x = tf.constant(features, tf.float64)
          targ = tf.constant(labels, tf.float64)
          a = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          b = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          # here I add a positivity constraint in a somewhat ad-hoc fashin
          # (restoring positivity of innovations at every timestep)
          y = a * x
          z = b * x
          z = y - tf.reduce_min(targ - y)
          loss = tf.losses.mean_squared_error(targ, z)
          opt = tf.train.GradientDescentOptimizer(learn).minimize(loss)
      with tf.Session(graph=gr) as s:
          tf.global_variables_initializer().run(session=s)
          for i in range(steps):
              r = s.run([opt, loss])
              if i % 100 == 0:
                  print(i, r)
          print(i, r)
          return s.run([b])
#+end_src
** TODO pysheds
https://github.com/mdbartos/pysheds

#+begin_src ipython :results silent :session
  from pysheds.grid import Grid

  grid = Grid.from_raster('/home/arno/Documents/data/hydro/DryCreek/DCEW-DEMclip.tif', 'dem')

  dirmap = (64, 128, 1, 2, 4, 8, 16, 32)
  grid.fill_depressions('dem', out_name='flooded_dem')
  grid.resolve_flats('flooded_dem', out_name='inflated_dem')
  grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)
  grid.catchment(data='dir', x=2, y=702, dirmap=dirmap, out_name='catch', recursionlimit=15000)
  grid.accumulation(data='catch', dirmap=dirmap, out_name='acc')
  grid.flow_distance(data='catch', x=2, y=702, dirmap=dirmap, out_name='dist')
#+end_src

#+begin_src ipython :results raw :session
  from cartopy.io.shapereader import Reader
  streams = Reader('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.shp')
  p = next(streams.geometries())
  GDAL.Affine(grid).ij(p.x, p.y)
#+end_src

#+RESULTS:
: [array(2), array(702)]
  
#+begin_src ipython :results raw :session :savefig phJ99M.png
  # import sys
  # sys.path.insert(1, '/home/arno/Documents/code/python/')
  # from data import GDAL
  # from plots import transparent as trnsp
  i, j = GDAL.coords(grid)
  proj = GDAL.proj2cartopy(grid.crs.srs)
  fig, axs = plt.subplots(1, 2, figsize=(8, 6), subplot_kw={'projection': proj})
  ax = axs[0]
  trnsp(ax.pcolormesh(i, j, GDAL.mv2nan(grid.dem, grid.nodata), cmap='cubehelix'))
  ax = axs[1]
  trnsp(ax.pcolormesh(i, j, np.log(GDAL.mv2nan(grid.acc, 0)), cmap='cubehelix'))
#+end_src

#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/phJ99M.png]]

#+begin_src ipython :results silent :session
  drainmap = np.array(list(dirmap))[(np.arange(8)+4) % 8]
  d = np.array(grid.dir)
  loc_acc = \
  ( d[:-2, 1:-1] == drainmap[0] ).astype(int) + \
  ( d[:-2, 2:]   == drainmap[1] ).astype(int) + \
  ( d[1:-1, 2:]  == drainmap[2] ).astype(int) + \
  ( d[2:, 2:]    == drainmap[3] ).astype(int) + \
  ( d[2:, 1:-1]  == drainmap[4] ).astype(int) + \
  ( d[2:, :-2]   == drainmap[5] ).astype(int) + \
  ( d[1:-1, :-2] == drainmap[6] ).astype(int) + \
  ( d[:-2, :-2]  == drainmap[7] ).astype(int)  
#+end_src

#+begin_src ipython :results raw :session :savefig pkfoOd.png
  proj = GDAL.proj2cartopy(grid.crs.srs)
  fig, ax = plt.subplots(figsize=(6, 4), subplot_kw={'projection': proj})
  pl = trnsp(ax.pcolormesh(i[1:-1, 1:-1], j[1:-1, 1:-1], loc_acc, cmap='cubehelix'))
  plt.colorbar(pl, ax=ax)
#+end_src

#+ATTR_ORG: :width 600
#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/pkfoOd.png]]

