* Note: this is not the same as the hydro.ipynb (yet)
* links
** http://www.hidro-limari.info/
** https://earth.boisestate.edu/drycreek/data/
* code
#+begin_src ipython :results silent :session
  import requests, os, re
  import pandas as pd
  import numpy as np
  from io import StringIO
  from functools import partial
  from data import DataConf, Unicode
  from dateutil.parser import parser as dparser

  class DryCreek(DataConf):
      file_name = Unicode().tag(config=True)
      column_names = {
          'T': ['AirTemperature-C', 'AirTemperature_C'],
          'nrad': ['NetRadiation-Watts/m2','NetRadiation_W/m2', 'NetRadiation_watts/m2'],
          'ppt': ['Precipitation-mm','Precipitation_mm'],
          'rh': ['RelativeHumidity-%', 'RelativeHumidity_percent'],
          'snwd': ['SnowDepth-cm', 'SnowDepth_cm'],
          'rs': ['SolarRadiation-Watts/m2','SolarRadiation_W/m2', 'SolarRadiation_watts/m2'],
          'dd': ['WindDirection-Degree', 'WindDirection_degree'],
          'vv': ['WindSpeed-m/s','WindSpeed_m/s'],
          'q': ['Discharge-L/s'],
          'ec': ['StreamEC-mS/cm'],
          'st': ['StreamT-C', 'StreamTemp-C']
      }
      base_url = 'http://icewater.boisestate.edu/boisefront-products/historical'
      urls = {
          'LG': 'dcew_lg/StreamHourlyData/LG_StreamHrlySummary_{:04d}.csv',
          'BSG': 'dcew_bsg/StreamHourlyData/BSG_StreamHrlySummary_{:04d}.csv',
          'C1E': 'dcew_c1e/StreamHourlyData/C1e_StreamHrlySummary_{:04d}.csv',
          'C1W': 'dcew_c1w/StreamHourlyData/C1W_StreamHrlySummary_{:04d}.csv',
          'C2E': 'dcew_c2e/StreamHourlyData/C2E_StreamHrlySummary_{:04d}.csv',
          'C2M': 'dcew_c2m/StreamHourlyData/C2M_StreamHrlySummary_{:04d}.csv',
          'LDP': 'dcew_ldp/Weather/Hourly%20Summary%20Data/LDP_HrlySummary_{:04d}.csv',
          'SCR': 'dcew_scr/weather/hourly%20summary%20data/SCR_HrlySummary_{:04d}.csv',
          'BRW': 'dcew_br/Weather/Hourly%20Summary%20Data/BRW_HrlySummary_{:04d}.csv',
          'LW': 'dcew_lw/Weather/Hourly%20Summary%20Data/LowerWeather_HrlySummary_{:04d}.csv',
          'TLW': 'dcew_tl/Weather/Hourly%20Summary%20Data/Treeline_HrlySummary_{:04d}.csv',
          'TLG': 'dcew_tl/Streamflow/TL_StreamHrlySummary_{:04d}.csv'
      }
      # (lat, lon, elev)
      geo = {
          'C2M': (43.7022778, -116.154095, 1143.),
          'C2E': (43.7027903, -116.1527722, 1158.),
          'C1W': (43.7184233, -116.1374192, 1347.),
          'C1E': (43.7181614, -116.1372986, 1335.),
          'BSG': (43.7407086, -116.0989983, 1680.),
          'BRW': (43.75876, -116.090404, 2114.),
          'LDP': (43.737078, -116.1221131, 1850.),
          'SCR': (43.71105, -116.09912, 1720.),
          'LW': (43.6885278, -116.16991, 1151.),
          'TL': (43.73019, -116.140143, 1610.)
      }

      def __init__(self, load=True, **kwargs):
          super().__init__(**kwargs)
          if load:
              self.load()

      @property
      def stream(self, data=False):
          pat = re.compile('stream', re.IGNORECASE)
          return [k for k, v in self.urls.items() if pat.search(v)]

      @property
      def met(self):
          pat = re.compile('weather', re.IGNORECASE)
          return [k for k, v in self.urls.items() if pat.search(v)]

      def get(self, station, years=range(1999, 2019), **kwargs):
          url = os.path.join(self.base_url, self.urls[station])
          f = partial(self._get_url, url, partial(self._parser, dparser()), **kwargs)
          self.data = [f(y) for y in years]
          self.data = pd.concat(self.data)
          self.data = self.data.loc[self.data.index.notnull()].sort_index()
          if len(self.data.columns.difference([i for j in self.column_names.values() for i in j])) > 0:
              raise Exception("Data has columns not present in column_names")
          else:
              self.merge(station)

      def get_all(self):
          for k in self.urls.keys():
              print("getting {}".format(k))
              self.get(k)

      def merge(self, name, data=None):
          data = self.data if data is None else data
          df = pd.concat([self._merge(data, n, c) for n, c in self.column_names.items()], 1).dropna(0, 'all')
          setattr(self, name, df.loc[df.index.notnull()])

      def store(self, name):
          getattr(self, name).to_hdf(os.path.join(self.path, self.file_name), name)

      @staticmethod
      def _get_url(url, date_parser, y, index_col='DateTime', na_values=[-6999, -6934, -7002.7, '#NUM!']):
          try:
              r = requests.get(url.format(y))
              r.raise_for_status()
          except:
              print('no data for year {}'.format(y))
              return
          print('year {}'.format(y))
          s = StringIO(r.text.replace('\r', '\n')) # some files had '\r' line endings
          while next(s)[:8] != index_col:
              p = s.seek(0, 1)
          s.seek(p)
          d = pd.read_csv(s, parse_dates=True, date_parser=date_parser, index_col=index_col, na_values=na_values)
          return d.loc[d.index.notnull()].dropna(1, 'all')

      @staticmethod
      def _parser(parser, s):
          try:
              return parser.parse(s)
          except:
              return np.nan

      @staticmethod
      def _merge(df, name, cols):
          c = df.columns.intersection(cols)
          if len(c) > 0:
              assert df[c].notnull().sum(1).max() == 1
              return df[c].sum(1).to_frame(name)

      def load(self):
          with pd.HDFStore(self.full_path) as S:
              for k in S.keys():
                  setattr(self, k[1:], S[k])

      def plot(self, *args, **kwargs):
          cm1 = plt.get_cmap('tab20b').colors
          cm2 = plt.get_cmap('tab20').colors
          colors = {
              'BRW': cm1[2],
              'BSG': cm1[3],
              'C1E': cm1[4],
              'LDP': cm1[5],
              'C1W': cm1[6],
              'SCR': cm1[9],
              'C2E': cm1[10],
              'C2M': cm1[14],
              'TLG': cm2[12],
              'TLW': cm2[13],
              'LW' : cm1[18],
              'LG' : cm1[19],
          }
          fig, ax = plt.subplots(*args, **kwargs)
          for i, k in enumerate(self.stream):
              try:
                  df = getattr(self, k)
              except: continue
              x = df['q']
              x.plot(ax=ax, label=k, color=colors[k])
          ax.legend(loc=2)

          bx = ax.twinx()
          for i, k in enumerate(self.met):
              try:
                  df = getattr(self, k)
              except: continue
              x = df['ppt'].dropna().diff()
              x = x[x>=0]
              x.plot(ax=bx, label=k, marker='*', ls='none', color=colors[k])
          bx.legend(loc=1)
#+end_src

#+begin_src ipython :results silent :session
  import requests, re
  from bs4 import BeautifulSoup as soup
  from io import StringIO
  import pandas as pd

  r = requests.get('https://earth.boisestate.edu/drycreek/data/lower-gage/')
  r.raise_for_status()
  s = soup(r.text, 'html.parser')
  def f(a):
      r = requests.get(a.attrs['href'])
      r.raise_for_status()
      return pd.read_csv(StringIO(r.text), skiprows=18, parse_dates=True, index_col='DateTime', na_values=-6999)

  df = pd.concat([f(a) for a in s.find_all('a', {'href': re.compile('HrlySummary')})]).dropna('all', 1)
#+end_src

#+begin_src ipython :results silent :session
  import pandas as pd
  import numpy as np
  # df is in ~/Documents/data/hydro/DryCreekBoise.h5
  df = pd.read_hdf('/home/arno/Documents/data/hydro/DryCreek/DryCreekBoise.h5', 'LG')
  df = df[df.index.notnull()].replace(-6934, np.nan).sort_index()
  d = df['2003':'2016'].iloc[:, 0] # no duplicates etc
  d = d['2011-08':'2014-04']       # no gaps
#+end_src

* Fourier Transforms
** idea was to test usefulness for gap filling
** not very useful, although maybe lsq or MEM approaches might offer more
*** part of the problem is the finite-length effect of the DFT (implied periodicity on the unit circle)
*** wavelets probably a much better idea
** attempt at a naive least-square implementation
#+begin_src ipython :results silent :session
  import numpy as np

  N = 256
  C = 16
  s = 1
  t = np.linspace(0, C, N)
  y = np.sin(2* np.pi * t) + np.random.rand(N) * s

  f = np.fft.fftfreq(N)
  f = f[f>0]

  x = 2 * np.pi * np.linspace(0, 1, N).reshape((-1, 1))
  X = x / f
  X = np.hstack((np.sin(X), np.cos(X)))
  l = np.linalg.lstsq(X, y)
#+end_src

** DFT by hand
*** to compare to just setting missing values to 0 (not tested yet)
#+begin_src ipython :results silent :session
  n = np.arange(N).reshape(-1, 1)
  n1 = np.r_[n[:100], n[110:]]
  k = np.arange(-N/8, N/8)
  F = np.exp(-2j * np.pi * n1 * k / N)
  G = np.exp(2j * np.pi * n * k / N)
  yh = G.dot(np.r_[y[:100], y[110:]].dot(F)) / N
#+end_src

* regressions etc
** tensorflow linear regression
#+begin_src ipython :results silent :session
  import tensorflow as tf

  def grdesc(features, labels, learn, steps):
      gr = tf.Graph()
      with gr.as_default():
          x = tf.placeholder(tf.float64)
          targ = tf.placeholder(tf.float64)
          a = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          b = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          # here I add a positivity constraint in a somewhat ad-hoc fashin
          # (restoring positivity of innovations at every timestep)
          y = a + b * x
          z = y - tf.reduce_min(targ - y)
          loss = tf.losses.mean_squared_error(targ, z)
          opt = tf.train.GradientDescentOptimizer(learn).minimize(loss)
      with tf.Session(graph=gr) as s:
          tf.global_variables_initializer().run(session=s)
          for i in range(steps):
              r = s.run([opt, loss], {x: features, targ: labels})
              if i % 100 == 0:
                  print(i, r)
          print(i, r)
          return s.run([a, b])
#+end_src

** BayesPy Bayesian linear regression
#+begin_src ipython :results silent :session
  import bayespy as bp
  from statsmodels.tools import add_constant

  class BayesLinReg(object):
      def __init__(self, x, steps=1000):
          self.B = bp.nodes.GaussianARD([0, 1], 1e-6, shape=(2,))
          self.F = bp.nodes.Dot(self.B, add_constant(x[:-1]))
          self.tau = bp.nodes.Gamma(1e-3, 1e-3)
          self.Y = bp.nodes.GaussianARD(self.F, self.tau)
          self.Y.observe(x[1:])
          self.Q = bp.inference.VB(self.Y, self.B, self.tau)
          self.Q.update(repeat=steps)
#+end_src

** Edward Bayesian linear regression
#+begin_src ipython :results silent :session
    import edward as ed
    import tensorflow as tf

    class BLM(object):
        def __init__(self, x, steps=500, K=2):
            self.gr = tf.Graph()
            tf.InteractiveSession(graph=self.gr)
            with self.gr.as_default():
                N = len(x) - 1
                X = tf.placeholder(tf.float32, [N])
                w = ed.models.Normal(loc=tf.ones([K, 1]), scale=tf.ones([K, 1]))

                r = ed.models.Normal(loc=tf.matmul(w, tf.expand_dims(X, 0)), scale=tf.ones(1))
                z = ed.models.Normal(loc=tf.ones((K, N)), scale=tf.ones(1))
                y = ed.models.Normal(loc=tf.reduce_sum(r * z, 0), scale=tf.ones(1))

                self.qw = ed.models.Normal(loc=tf.get_variable("qw/loc", [K, 1]),
                                           scale=tf.nn.softplus(tf.get_variable("qw/scale", [K, 1])))
                self.qz = ed.models.Normal(loc=tf.get_variable("qz/loc", [K, N]),
                                           scale=tf.nn.softplus(tf.get_variable("qz/scale", [1])))
                self.infer = ed.KLqp({w: self.qw, z: self.qz}, data={
                    X: x[:-1],
                    y: x[1:]
                })
                self.infer.run(n_samples=5, n_iter=steps)
#+end_src


* Ad-hoc analyses
#+begin_src ipython :results silent :session
  from data import GDAL
  from cartopy.io.shapereader import Reader
  DEM = GDAL.GeoTiff('/home/arno/Documents/data/hydro/DryCreek/DCEW-DEMclip.tif')
  stream_vec = Reader('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.shp')
  stream_raster = GDAL.GeoTiff('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.tif')
#+end_src

#+begin_src ipython :results raw :session :savefig catchment.png
  fig, ax = plt.subplots(subplot_kw={'projection': DEM.cartopy}, figsize=(8, 8))
  DEM.pcolormesh(ax, background={}, cmap='terrain')
  stream_raster.pcolormesh(ax, background={}, cmap='Dark2')
  px, py = zip(*[(p.x, p.y) for p in stream_vec.geometries()])
  ax.scatter(px, py, facecolor='r')
#+end_src

#+ATTR_ORG: :width 600
#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/catchment.png]]

#+begin_src ipython :results raw :session :savefig alpha_hist.png
  x = d.values
  z = x[1:] / x[:-1]
  fig, axs = plt.subplots(1, 2, figsize=(12, 5))
  ax = axs[0]
  h = ax.hist(z[(z>.98) & (z<1.02)], 1000, color='chartreuse')
  ax.set_xlim(.98, 1.02)
  ax.set_ylim(0, 50)

  z = np.log(z)
  z = 1 / z[z!=0]
  ax = axs[1]
  ax.hist(z[np.isfinite(z)], 1000, color='orangered')
  ax.set_xlim(-150, 150)
  # ax.set_ylim(0, 200)
#+end_src


#+CAPTION: Histograms of "local AR(1) coefficients": x_k / x_{k-1}, where x_k refers to any index in the discharge time series. Left: raw (count on 1 is 14691, ~60% of data). Right: transformed as 1 / log(alpha), which corresponds the e-folding time (in basic units of the timeseries, which is 1h) of an exponential growth/decay process.

#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/alpha_hist.png]]

#+begin_src ipython :results raw :session
  # count in bin centered on one (which is the max count)
  np.max(h[0])
#+end_src

#+RESULTS:
: 14691.0

** Notes
*** Computing the quotient timeseries $\alpha_k = q_{k+1} / q_k$ can be seen as computing a local autoregressive ($AR(1)$) coefficient. While the individual coefficients are not very informative, its distribution is.
*** Furthermore, the inverse of the log of the quotient series ($ln \alpha_k^{-1}$) gives the corresponding e-folding time of a moving-average impulse response.
*** The histograms clearly show 2-3 modes, corresponding to 2-3 major subbasins of the Dry Creek catchment.
*** $\alpha_k < 1$ (or $ln \alpha_k^{-1}<0$) corresponds to the receeding limb of the hydrograph, whereas values >1 (>0) correspond to the ascending limb
*** However, in terms of time series modeling, $\alpha_k>1$ are unphysical; this would imply water levels increasing by themselves rather than because of rainfall.
*** Rainfall would correspond to the innovations of the AR / MA process, but with $\alpha_k$s below 1
*** Note also that the quotient series is very unstable and has very large values at times of very low flow.
**** Some form of regularization/smoothing is necessary for anything more informative.
*** The e-folding time will be influenced by (???):
**** the distance the water has to travel
**** the intensity of the rain

#+begin_src ipython :results silent :session
  def grdesc(features, labels, learn, steps):
      gr = tf.Graph()
      with gr.as_default():
          x = tf.constant(features, tf.float64)
          targ = tf.constant(labels, tf.float64)
          a = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          b = tf.Variable(tf.random_normal([1], dtype=tf.float64), dtype=tf.float64)
          # here I add a positivity constraint in a somewhat ad-hoc fashin
          # (restoring positivity of innovations at every timestep)
          y = a * x
          z = b * x
          z = y - tf.reduce_min(targ - y)
          loss = tf.losses.mean_squared_error(targ, z)
          opt = tf.train.GradientDescentOptimizer(learn).minimize(loss)
      with tf.Session(graph=gr) as s:
          tf.global_variables_initializer().run(session=s)
          for i in range(steps):
              r = s.run([opt, loss])
              if i % 100 == 0:
                  print(i, r)
          print(i, r)
          return s.run([b])
#+end_src
** pysheds
https://github.com/mdbartos/pysheds

#+begin_src ipython :results silent :session
  from pysheds.grid import Grid

  grid = Grid.from_raster('/home/arno/Documents/data/hydro/DryCreek/DCEW-DEMclip.tif', 'dem')

  dirmap = (64, 128, 1, 2, 4, 8, 16, 32)
  grid.fill_depressions('dem', out_name='flooded_dem')
  grid.resolve_flats('flooded_dem', out_name='inflated_dem')
  grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)
  grid.catchment(data='dir', x=2, y=702, dirmap=dirmap, out_name='catch', recursionlimit=15000)
  grid.accumulation(data='catch', dirmap=dirmap, out_name='acc')
  grid.flow_distance(data='catch', x=2, y=702, dirmap=dirmap, out_name='dist')
#+end_src

#+begin_src ipython :results raw :session
  from cartopy.io.shapereader import Reader
  streams = Reader('/home/arno/Documents/data/hydro/DryCreek/streamIDs1000.shp')
  p = next(streams.geometries())
  GDAL.Affine(grid).ij(p.x, p.y)
#+end_src

#+RESULTS:
: [array(2), array(702)]
  
#+begin_src ipython :results raw :session :savefig phJ99M.png
  # import sys
  # sys.path.insert(1, '/home/arno/Documents/code/python/')
  # from data import GDAL
  # from plots import transparent as trnsp
  i, j = GDAL.coords(grid)
  proj = GDAL.proj2cartopy(grid.crs.srs)
  fig, axs = plt.subplots(1, 2, figsize=(8, 6), subplot_kw={'projection': proj})
  ax = axs[0]
  trnsp(ax.pcolormesh(i, j, GDAL.mv2nan(grid.dem, grid.nodata), cmap='cubehelix'))
  ax = axs[1]
  trnsp(ax.pcolormesh(i, j, np.log(GDAL.mv2nan(grid.acc, 0)), cmap='cubehelix'))
#+end_src

#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/phJ99M.png]]

#+begin_src ipython :results silent :session
  drainmap = np.array(list(dirmap))[(np.arange(8)+4) % 8]
  d = np.array(grid.dir)
  loc_acc = \
  ( d[:-2, 1:-1] == drainmap[0] ).astype(int) + \
  ( d[:-2, 2:]   == drainmap[1] ).astype(int) + \
  ( d[1:-1, 2:]  == drainmap[2] ).astype(int) + \
  ( d[2:, 2:]    == drainmap[3] ).astype(int) + \
  ( d[2:, 1:-1]  == drainmap[4] ).astype(int) + \
  ( d[2:, :-2]   == drainmap[5] ).astype(int) + \
  ( d[1:-1, :-2] == drainmap[6] ).astype(int) + \
  ( d[:-2, :-2]  == drainmap[7] ).astype(int)  
#+end_src

#+begin_src ipython :results raw :session :savefig pkfoOd.png
  proj = GDAL.proj2cartopy(grid.crs.srs)
  fig, ax = plt.subplots(figsize=(6, 4), subplot_kw={'projection': proj})
  pl = trnsp(ax.pcolormesh(i[1:-1, 1:-1], j[1:-1, 1:-1], loc_acc, cmap='cubehelix'))
  plt.colorbar(pl, ax=ax)
#+end_src

#+ATTR_ORG: :width 600
#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/pkfoOd.png]]
#+begin_src ipython :results silent :session
  import pandas as pd
  import matplotlib.pyplot as plt

  dc = DryCreek()
#+end_src

#+begin_src ipython :results silent :session
  ldp = pd.read_hdf(dc.full_path, 'LDP')['ppt'] # Lower Deer Point (N basin)
  scr = pd.read_hdf(dc.full_path, 'SCR')['ppt'] # Shingle Creek Ridge (S basin)

  a = (lambda x: x[x>=0])(ldp.dropna().diff())
  b = (lambda x: x[x>=0])(scr.dropna().diff())
  anan = (lambda x: x.index[x.isnull()])(a.loc[b.index])
  bnan = (lambda x: x.index[x.isnull()])(b.loc[a.index])
#+end_src


#+begin_src ipython :results silent :session
  a1 = (lambda x: x>10)(a/b)
  b1 = (lambda x: x>10)(b/a)
  a10 = a[a1].sort_values(ascending=False)[:10]
  b10 = b[b1].sort_values(ascending=False)[:10]
#+end_src


#+begin_src ipython :results raw :session
a10
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
  DateTime
  2017-01-11 09:00:00    49.7
  2018-01-05 15:00:00    31.8
  2017-06-02 14:00:00    17.5
  2012-04-26 04:00:00    10.2
  2017-01-11 05:00:00     8.5
  2018-02-27 11:00:00     7.1
  2010-04-20 23:00:00     6.8
  2014-02-10 10:00:00     6.7
  2011-05-26 09:00:00     6.2
  2010-06-04 09:00:00     5.6
  Name: ppt, dtype: float64
#+END_EXAMPLE

#+begin_src ipython :results raw :session
b10
#+end_src

#+RESULTS:
#+BEGIN_EXAMPLE
  DateTime
  2016-03-30 14:00:00    97.0
  2016-10-22 14:00:00    45.9
  2017-11-01 17:00:00    25.6
  2016-05-07 18:00:00    16.9
  2015-12-20 12:00:00    15.3
  2016-05-06 20:00:00    13.0
  2016-02-22 13:00:00    11.9
  2014-12-21 00:00:00    10.7
  2017-03-30 07:00:00     7.6
  2011-06-07 02:00:00     7.2
  Name: ppt, dtype: float64
#+END_EXAMPLE

#+begin_src ipython :results silent :session
  q = pd.read_hdf(dc.full_path, 'LG').iloc[:, 0]
  c2m = pd.read_hdf(dc.full_path, 'C2M').iloc[:, 0]
  c2e = pd.read_hdf(dc.full_path, 'C2E').iloc[:, 0]
  c1w = pd.read_hdf(dc.full_path, 'C1W').iloc[:, 0]
  c1e = pd.read_hdf(dc.full_path, 'C1E').iloc[:, 0]
  bsg = pd.read_hdf(dc.full_path, 'BSG').iloc[:, 0]
#+end_src



#+begin_src ipython :results raw :session
  import matplotlib.pyplot as plt
  import cmocean as cmo

  q_clrs = cmo.cm.deep(np.linspace(0, 1, 5))
  r_clrs = plt.get_cmap('Paired').colors
  # fig, axs = plt.subplots(1, 2, figsize=(8, 4))
  def plot(ax):
      a.plot(color=r_clrs[1], ax=ax, label='LDP')
      b.plot(ax=ax, color=r_clrs[6], label='SCR')
      ax.plot(anan, b.loc[anan], '^', color=r_clrs[1])
      ax.plot(bnan, a.loc[bnan], 'v', color=r_clrs[6])
      ax.legend(loc=2)
      bx = ax.twinx()
      bx.plot(q, color='c', label='LG')
      bx.plot(c1e, color=r_clrs[0], label='C1E')
      bx.plot(c2m, color=r_clrs[5], label='C2M')
      bx.plot(c2e, color=r_clrs[7], label='C2E')
      bx.plot(bsg, color=r_clrs[2], label='BSG')
      bx.legend(loc=1)
  # for ax in axs:
  #     plot(ax)
  # axs[0].set_xlim('2017-01', '2017-02')
  # axs[1].set_xlim('2016-10', '2016-12')
#+end_src

#+RESULTS:

Local Variables:
eval: (switch-pyvenv "hydro")
End:
** 04-30
*** data preparation
#+begin_src ipython :results silent :session
  dc = DryCreek()
  dc.load()

  d = [getattr(dc, k)['q'] for k in dc.stream]
  d[0] = d[0].loc['2003':] # duplicates in LG
  df = pd.concat(d, 1)
  df.columns = dc.stream
#+end_src

#+begin_src ipython :results silent :session
  r = pd.concat([getattr(dc, k)['ppt'] for k in dc.met], 1)
  r.columns = dc.met
  r = r.diff()
  r[r<0] = 0
#+end_src
*** K_means
#+begin_src ipython :results silent :session
  from sklearn import cluster
  from scipy.stats import binned_statistic

  q = x.AR.as_matrix()
  idx = (1<q) & (q<1.02)
  q = q[idx]
  km = cluster.KMeans(n_clusters=6)
  km.fit(q.reshape(-1, 1))
  hq, bins, _ = binned_statistic(q, km.labels_, bins=1000)
#+end_src
*** histogram with colored bars
#+begin_src ipython :results silent :session
  def chist(q, hq, bins):
      colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
      plt.figure()
      _, _, patches = plt.hist(q, bins)
      for p, h in zip(patches, hq):
          p.set_facecolor(colors[int(round(h))])
#+end_src
*** now
#+begin_src ipython :results silent :session
  idx = (df.C2E>0) & (df.C2M>100)
  ew = (df.C2E / df.C2M)[idx]
  ar = (df.C2M / df.C2M.shift(1))[idx]
#+end_src


#+begin_src ipython :results raw :session :savefig quotient_vs_branch.png
  fig, ax = plt.subplots(figsize=(8, 6))
  ax.scatter(ew, ar, marker='.', color='orangered')
  ax.set_xlabel('C2E / C2W')
  ax.set_ylabel('q[t] / q[t-1]')
#+end_src

#+CAPTION: Quotient discharge at C2M vs quotient of C2E / C2M: higher x-axis value means more flow from eastern branch (C2E), whereas higher y-axis means higher AR(1) coefficient. Low flow values have been removed (<= 100).
#+ATTR_ORG: :width 600 
#+RESULTS:
[[/home/arno/Documents/code/notebooks/obipy-resources/hydro/quotient_vs_branch.png]]

